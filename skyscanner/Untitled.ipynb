{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d398a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from auto_gptq import AutoGPTQForCausalLM\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer , GenerationConfig , TextStreamer , pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b29a16eb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1020104395.py, line 34)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [3]\u001b[1;36m\u001b[0m\n\u001b[1;33m    ai_prefix=\"### Response\",\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "DEFAULT_TEMPLATES = \"\"\"\n",
    "### instruction: You're a travelling support agent that is talking to a customer . Use only the caht history and the following information \n",
    "{context}\n",
    "to answer in a helpful manner to the queries . If you don't know the answer - say that you don't know.\n",
    "Keep your replies short , compassionate and informative .\n",
    "{chat_history}\n",
    "### Input:{question}\n",
    "### Response :\"\"\".strip()\n",
    "\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_pipeline:HuggingFacePipeline,\n",
    "        embeddings:HuggingFaceEmbeddings,\n",
    "        documents_sir:Path,\n",
    "        prompt_template:ste=DEFAULT_TEMPLATE,\n",
    "        verbrose:bool = False,):\n",
    "        \n",
    "        prompt = PromptTemplate(\n",
    "        input_variable = [\"context\",\"question\",\"chat_history\"],\n",
    "        template=prompt_template,)\n",
    "        \n",
    "        self.chain =self.create_chain(text_pipeline,prompt,verbrose)\n",
    "        self.db = self.embed_data(documents_dir,embeddings)\n",
    "        \n",
    "    def _create_chain(self,\n",
    "                     text_pipeline:HuggingFacePipeline,\n",
    "                     prompt = PromptTemplate,\n",
    "                     verbrose:bool = False,):\n",
    "        memory = ConversationBufferMemory(\n",
    "                 memory_key='chat_history',\n",
    "                 human_prefix=\"### Input\"\n",
    "                 ai_prefix=\"### Response\",\n",
    "                 input_key =\"question\",\n",
    "                 output_key=\"output_text\",\n",
    "                 return_message=False\n",
    "                 )\n",
    "        return load_qa_chain(\n",
    "               text_pipeline,\n",
    "               chain_type=\"stuff\",\n",
    "               prompt=prompt,\n",
    "               memory=memory,\n",
    "               verbrose=verbrose)\n",
    "    def _embed_data(\n",
    "         self,documents_dir:Path,embedings:HuggingFaceEmbeddings)->Chroma:\n",
    "        loader =DirectoryLoader(documnet_dir,glob=\"**/*txt\")\n",
    "        documents =loader.load()\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=512,chunk_overlap=0)\n",
    "        texts = texts_splitter.split_documents(documents)\n",
    "        return Chroma.from_documents(texts,embeddings)\n",
    "    def __call__(self,user_input:str)->str:\n",
    "        docs = self.db.similarity_search(user_input)\n",
    "        return self.chain.run({\"input_documents\":docs,\"questions\":user_input})\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c341c384",
   "metadata": {},
   "source": [
    "# Coversational Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "089ed34d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m streamer \u001b[38;5;241m=\u001b[39m TextStreamer(\u001b[43mtokenizer\u001b[49m,skip_prompt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,use_multiprocessing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "streamer = TextStreamer(tokenizer,skip_prompt=True,skip_special_tokens=True,use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc47a90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to gpt2 and revision 6c0e608 (https://huggingface.co/gpt2).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4488a525f734678b8a26dba281412a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\subra\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\subra\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c976079e4beb480fa322fac90bf63656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e605add866f484e90c90d1291cbe8d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)neration_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f93fc96c42744fda66c4d9c5c68f854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b8f42e952dd4ceca3bd0eaddca5ca90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fbb97c2f76e4706abafc15af08dd325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pipe = pipeline('text-generation',\n",
    "#                 model=model,\n",
    "                max_length = 2048,\n",
    "                temperature=0.5,\n",
    "                top_p=0.95,\n",
    "#                 streamer=streamer\n",
    "#                 generation_config=generation_config,\n",
    "                batch_size=1,\n",
    "                repetation_penalty=1.15,\n",
    "               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab539dbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebc0d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9f353f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3e3265",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7e98a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "### instruction: You're a travelling support agent that is talking to a customer . Use only the caht history and the following information \n",
    "{context}\n",
    "to answer in a helpful manner to the queries . If you don't know the answer - say that you don't know.\n",
    "Keep your replies short , compassionate and informative .\n",
    "{chat_history}\n",
    "### Input:{question}\n",
    "## Respose\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38024c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "        input_variables = [\"context\",\"question\",\"chat_history\"],template=template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f710c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        human_prefix=\"##Input\",\n",
    "        ai_prefix = \"### Response\",\n",
    "        output_key=\"answer\",\n",
    "        return_messages=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "63a67c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chain = ConversationalRetrievalChain.from_llm(\n",
    "#         pipe,\n",
    "#         chain_type=\"stuff\",\n",
    "#         retriever=db.as_retriever(),\n",
    "#         memory =memory,\n",
    "#         combine_docs_chain_kwargs={'prompt':prompt},\n",
    "#         return_source_documents=True,\n",
    "#         verbrose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "948bd7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# translator = pipeline('text-generation',model=\"TheBloke/Nous-Hermes-13B-GPTQ\",device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b24bc5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a283d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from transformers import AutoTokenizer , GenerationConfig , TextStreamer , pipeline , TFAutoModelForQuestionAnswering\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f52c951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a916a8c4fa014cc8a6cb0cd3dc2de0a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243c7aebf8be481a84dd7be233efd622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/451 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6402c2841124886ab8ed982019fb441",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c29b566ddd64d9fac648537f2d0908d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ca8e842e11d446e97bd94e647b2b413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/265M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
      "\n",
      "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-distilled-squad\")\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"distilbert-base-uncased-distilled-squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c15ecea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question:How do I search for multiple destinations?\n",
      "Answer: simply click the ' multi - city ' button at the start of your search\n"
     ]
    }
   ],
   "source": [
    "context= \"For a multi-city search on Skyscanner, simply click the 'Multi-city' button at the start of your search. From there, you can input your desired travel dates, destinations, and additional flights. Although the price alert option isn't currently available for multi-city searches, the Skyscanner team is actively working on introducing this feature in the near future. To learn more about how to navigate this new feature, refer to the provided article. Skyscanner values user feedback to enhance their services, and you can share your thoughts by clicking the 'Contact Us' button below the article.\"\n",
    "qn =[\"How do I search for multiple destinations?\"]\n",
    "\n",
    "for question in qn:\n",
    "    inputs = tokenizer(question,context,add_special_tokens=True,return_tensors='tf')\n",
    "    input_ids=inputs[\"input_ids\"].numpy()[0]\n",
    "    \n",
    "    outputs = model(inputs)\n",
    "    answer_start_scores = outputs.start_logits\n",
    "    answer_end_scores = outputs.end_logits\n",
    "    \n",
    "    answer_start =tf.argmax(\n",
    "    answer_start_scores,axis=1).numpy()[0]\n",
    "    \n",
    "    answer_end = (tf.argmax(answer_end_scores,axis=1)+1).numpy()[0]\n",
    "    answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n",
    "    print(f\"question:{question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7511db77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
